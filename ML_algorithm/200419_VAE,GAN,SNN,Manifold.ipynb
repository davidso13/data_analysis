{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"200419_VAE,GAN,SNN,Manifold.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP6NU0z7Bbotwex4J7v5yDH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fLGdH4InYWUZ","colab_type":"text"},"source":["## 200419 머신러닝 알고리즘 공부"]},{"cell_type":"code","metadata":{"id":"eDt55O9LbHZ3","colab_type":"code","outputId":"ddb1ab23-3474-42ab-b2ba-da506a737072","executionInfo":{"status":"ok","timestamp":1587266669171,"user_tz":-540,"elapsed":18894,"user":{"displayName":"david song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil3Fx21Bq7GBVRGaEz4SijRJXcHDbCf-XMuKGf6Q=s64","userId":"03748752257933305700"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append(\"/content/gdrive/My Drive/Colab Notebooks\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-3Q-lZ4NYWYJ","colab_type":"text"},"source":["## 1. VAE(Variational Autoencoder)"]},{"cell_type":"markdown","metadata":{"id":"B-11_RfSmGoK","colab_type":"text"},"source":["### - Autoencoder"]},{"cell_type":"markdown","metadata":{"id":"3spC4npHmzpY","colab_type":"text"},"source":["- 차원축소(hidder layer의 뉴런 수 < input layer의 뉴런수) or 입력 데이터에 노이즈를 추가한 후 원본 입력을 복원할 수 있도록 네트워크 학습시키는 등 다양한 오토인코더가 존재하다.\n","- 입력층과 출력층의 뉴런 수가 동일하다. <br> <br>\n","1. Encoder: 입력을 내부 표현으로 변환(= recognition network)\n","2. Decoder: 내부 표현을 출력으로 변환(= generative network)"]},{"cell_type":"markdown","metadata":{"id":"ctyJTbgsmqb2","colab_type":"text"},"source":["![대체 텍스트](https://drive.google.com/uc?id=14tXPClV3DNaKD0NChdvv8s6SEPaJYbQi)"]},{"cell_type":"markdown","metadata":{"id":"dIlmFYGAnlfJ","colab_type":"text"},"source":["### - Concept of VAE"]},{"cell_type":"markdown","metadata":{"id":"HlRLsSeemGqk","colab_type":"text"},"source":["- Generative 모델의 일종\n","\n","![대체 텍스트](https://drive.google.com/uc?id=1ENzJTcff0g0gwkcXOpfRnf_aE5ugJKmd)"]},{"cell_type":"markdown","metadata":{"id":"Zw42LsdWmGsj","colab_type":"text"},"source":["1. Encoder: 관측된 데이터 x로 부터 잠재변수 z를 가정\n","2. Decoder: 잠재변수 z를 활용해 x를 복원해내는 역할"]},{"cell_type":"markdown","metadata":{"id":"foOxnRM9pVZ5","colab_type":"text"},"source":["#### - Variational interface"]},{"cell_type":"markdown","metadata":{"id":"NjKl5SB9pX7x","colab_type":"text"},"source":["- 이상적인 확률분포를 모르는 상태에서, 다루기 쉬운 분포(ex. Gaussian Distribution)를 가정하고, 이 확률분포의 모수(평균, 표준편차)를 바꿔가며 이상적인 확률분포에 근사하게 만들어 그 확률분포를 대신 사용하는 것이다."]},{"cell_type":"markdown","metadata":{"id":"GjxRR7svq8LY","colab_type":"text"},"source":["### - Generative 모델\n","1. Pixel CNN / Pixel RNN\n","2. Variational AutoEncoder(VAE)\n","3. Gaenerative Adversial Network(GAN)"]},{"cell_type":"markdown","metadata":{"id":"j-koXLgfoSc_","colab_type":"text"},"source":["### - 예제 코드"]},{"cell_type":"code","metadata":{"id":"GWs-preXqGAS","colab_type":"code","colab":{}},"source":["def __init__(self):\n","    self.fc1_1 = nn.Linear(784, hidden_size)\n","    self.fc1_2 = nn.Linear(784, hidden_size)\n","    self.relu = nn.ReLU()\n","                        \n","def encode(self,x):\n","    x = x.view(batch_size,-1)\n","    mu = self.relu(self.fc1_1(x))\n","    log_var = self.relu(self.fc1_2(x))\n","    return mu,log_var\n","    \n","def reparametrize(self, mu, logvar):\n","    std = logvar.mul(0.5).exp_()\n","    eps = torch.FloatTensor(std.size()).normal_()\n","    return eps.mul(std).add_(mu)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kz-fTrJHoSfn","colab_type":"text"},"source":["* Reference <br>\n","1. https://ratsgo.github.io/generative%20model/2018/01/27/VAE/ <br<>\n","2. https://datascienceschool.net/view-notebook/c5248de280a64ae2a96c1d4e690fdf79/"]},{"cell_type":"markdown","metadata":{"id":"iQtth2-cYWah","colab_type":"text"},"source":["## 2. GAN(Generative Adversarial Network)\n"]},{"cell_type":"markdown","metadata":{"id":"hJ-b9ITLdCvF","colab_type":"text"},"source":["- 경찰:진짜, 가짜 화폐를 판별(분류)하여 위조지폐범을 검거하는 것이 목표 ; 분류모델<br>\n","- 위조지폐범: 최대한 진짜 같은 화폐 만들어(생성) 경찰 속이기 ; 생성모델 <br> <br>\n","- 서로가 서로를 경쟁적으로 발전시키는 구조"]},{"cell_type":"markdown","metadata":{"id":"k1wFkmv6edQ3","colab_type":"text"},"source":["![대체 텍스트](https://drive.google.com/uc?id=10nojco6KGzT3bk0vPZbmzZPVk1-f76C9)"]},{"cell_type":"markdown","metadata":{"id":"mbHOcTkafCM0","colab_type":"text"},"source":["![대체 텍스트](https://drive.google.com/uc?id=1EfgX9dvbSKo7Xn9J1-7b22qdvmy58GyH)"]},{"cell_type":"markdown","metadata":{"id":"Xm14e_WshQ0t","colab_type":"text"},"source":["### - 구체적인 방법: minmax problem"]},{"cell_type":"markdown","metadata":{"id":"p6vBRczVhXKa","colab_type":"text"},"source":["![대체 텍스트](https://drive.google.com/uc?id=1xozI6C8PGdNsi2wApukKno9_KjrdX6Rv)"]},{"cell_type":"markdown","metadata":{"id":"HcIu8AYOhlqE","colab_type":"text"},"source":["\n","- x~Pdata(x): 실제 데이터 확률분포에서 샘플링한 데이터\n","- z~Pz(z): 가우시안 분포를 사용하는 임의의 노이즈에서 샘플링한 데이터\n","- D(x): 분류자, 데이터가 진짜면 D(x)는 1, 가짜면 0의 값을 내놓는다.\n","- D(G(z)): G가 만들어내는 데이터인 G(z)가 진짜면 1, 가짜면 0의 값을 가짐."]},{"cell_type":"markdown","metadata":{"id":"IZ4OyZUxhrba","colab_type":"text"},"source":["#### 1. Discriminator: V(D,G) 최대화\n","- 두 항 모두 최대화\n","- log D(x), 1-D(G(z)) 모두 최대화\n","- D(x) = 1, D(G(z)) = 0\n","- 진짜 데이터는 진짜로 분류, 생성자가 만들어 낸 가짜 데이터는 가짜라고 분류 <br><br>\n","\n","\n","#### 2. Generator: V(D,G) 최소화\n","- 두 항 모두 최소화\n","- log D(x), 1-D(G(z)) 모두 최소화\n","- D(x) = 0, D(G(z)) = 1\n","- 가짜 데이터를 진짜로 분류할만큼 학습\n"]},{"cell_type":"markdown","metadata":{"id":"qQGoxSRgi6Jc","colab_type":"text"},"source":["### - GAN의 한계점"]},{"cell_type":"markdown","metadata":{"id":"GLm_-mM8jXkC","colab_type":"text"},"source":["1. 기존 GAN만으로는 훈련 성능이 좋지 않음. - 균형 있게 훈련을 주고 받지 못하는 경우 존재\n","2. 사용된 생성자의 결과물 형태가 어떠한 과정을 통해 나왔는지 알 수 없다.\n","3. 새롭게 만들어진 데이터가 얼마나 정확하지 객관적으로 판단할 수 없다."]},{"cell_type":"markdown","metadata":{"id":"y1aP_v3VjyvM","colab_type":"text"},"source":["### - DCGAN"]},{"cell_type":"markdown","metadata":{"id":"jAIE3ngBj1J5","colab_type":"text"},"source":["GAN에 CNN을 사용해 불균형한 훈련 차이를 줄이는 방법"]},{"cell_type":"markdown","metadata":{"id":"dDZYwlMhkDBy","colab_type":"text"},"source":["![대체 텍스트](https://drive.google.com/uc?id=1tsjEUsYlaCd2omctNno6icgUNGlCAyJu)"]},{"cell_type":"markdown","metadata":{"id":"BnIOa0_lkToa","colab_type":"text"},"source":["### - 활용사례"]},{"cell_type":"markdown","metadata":{"id":"Rz4EexirkVJ5","colab_type":"text"},"source":["1. Fake Obama <br>\n","https://www.youtube.com/watch?v=AmUC4m6w1wo <br><br>\n","2. Real-eye-opener by facebook <br>\n","- (a),(b) : 실제사진 <br>\n","- (c) : 포토샾 합성 <br>\n","- (d) : GAN\n","\n","![대체 텍스트](https://drive.google.com/uc?id=18GO8HLYI-L1TCEX6yM-nDdMoNCy1RCRc)\n","\n","<br><br>\n","\n","3. Image Translation\n","![대체 텍스트](https://drive.google.com/uc?id=1z78332NAKD8btfK6cGIpNug46XJIxEFZ)"]},{"cell_type":"markdown","metadata":{"id":"FapKX11Fl53p","colab_type":"text"},"source":["### - 코드 예제"]},{"cell_type":"code","metadata":{"id":"90qI-_AKl44h","colab_type":"code","outputId":"58f49c22-bf54-4949-9926-fae5c8e556fa","executionInfo":{"status":"error","timestamp":1587268806906,"user_tz":-540,"elapsed":830,"user":{"displayName":"david song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil3Fx21Bq7GBVRGaEz4SijRJXcHDbCf-XMuKGf6Q=s64","userId":"03748752257933305700"}},"colab":{"base_uri":"https://localhost:8080/","height":352}},"source":["class GAN():\n","    def __init__(self):\n","        self.img_rows = 28\n","        self.img_cols = 28\n","        self.channels = 1\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","\n","        optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss='binary_crossentropy',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        # Build and compile the generator\n","        self.generator = self.build_generator()\n","        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","        # The generator takes noise as input and generated imgs\n","        z = Input(shape=(100,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The valid takes generated images as input and determines validity\n","        valid = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator) takes\n","        # noise as input => generates images => determines validity\n","        self.combined = Model(z, valid)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","    def build_generator(self):\n","\n","        noise_shape = (100,)\n","\n","        model = Sequential()\n","\n","        model.add(Dense(256, input_shape=noise_shape))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Dense(512))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Dense(1024))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n","        model.add(Reshape(self.img_shape))\n","\n","        model.summary()\n","\n","        noise = Input(shape=noise_shape)\n","        img = model(noise)\n","\n","        return Model(noise, img)\n","\n","    def build_discriminator(self):\n","\n","        img_shape = (self.img_rows, self.img_cols, self.channels)\n","\n","        model = Sequential()\n","\n","        model.add(Flatten(input_shape=img_shape))\n","        model.add(Dense(512))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dense(256))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dense(1, activation='sigmoid'))\n","        model.summary()\n","\n","        img = Input(shape=img_shape)\n","        validity = model(img)\n","\n","        return Model(img, validity)\n","\n","    def train(self, epochs, batch_size=128, save_interval=50):\n","\n","        # Load the dataset\n","        (X_train, _), (_, _) = mnist.load_data()\n","\n","        # Rescale -1 to 1\n","        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n","        X_train = np.expand_dims(X_train, axis=3)\n","\n","        half_batch = int(batch_size / 2)\n","\n","        for epoch in range(epochs):\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            # Select a random half batch of images\n","            idx = np.random.randint(0, X_train.shape[0], half_batch)\n","            imgs = X_train[idx]\n","\n","            noise = np.random.normal(0, 1, (half_batch, 100))\n","\n","            # Generate a half batch of new images\n","            gen_imgs = self.generator.predict(noise)\n","\n","            # Train the discriminator\n","            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","\n","            noise = np.random.normal(0, 1, (batch_size, 100))\n","\n","            # The generator wants the discriminator to label the generated samples\n","            # as valid (ones)\n","            valid_y = np.array([1] * batch_size)\n","\n","            # Train the generator\n","            g_loss = self.combined.train_on_batch(noise, valid_y)\n","\n","            # Plot the progress\n","            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","\n","            # If at save interval => save generated image samples\n","            if epoch % save_interval == 0:\n","                self.save_imgs(epoch)\n","\n","    def save_imgs(self, epoch):\n","        r, c = 5, 5\n","        noise = np.random.normal(0, 1, (r * c, 100))\n","        gen_imgs = self.generator.predict(noise)\n","\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(\"gan/images/mnist_%d.png\" % epoch)\n","        plt.close()\n","\n","\n","if __name__ == '__main__':\n","    gan = GAN()\n","    gan.train(epochs=30000, batch_size=32, save_interval=200)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-a8a68e536bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-a8a68e536bd6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Build and compile the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Adam' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"q5qnAoATi6Qx","colab_type":"text"},"source":["* Referenece \n","1. https://www.samsungsds.com/global/ko/support/insights/Generative-adversarial-network-AI-2.html <br>\n","2. https://www.samsungsds.com/global/ko/support/insights/Generative-adversarial-network-AI-3.html"]},{"cell_type":"markdown","metadata":{"id":"QQCblSCnazXQ","colab_type":"text"},"source":["## 3. Spiking Neural Network(SNN)"]},{"cell_type":"markdown","metadata":{"id":"pxLahB5orfCW","colab_type":"text"},"source":["### - 도입 배경"]},{"cell_type":"markdown","metadata":{"id":"wv6536PHrmov","colab_type":"text"},"source":["1. 신경망: 탁원할 결과 but 학습 시 막대한 계산량 필요.\n","2. 좀 더 생물학적인 방법으로 신경망을 구현하고자 하는 노력 -> SNN\n","3. PI-MNIST 문제에 대해서 기존 ANN 모델이 99.06%, SNN 모델이 98.77%의 정확도를 보임.\n","4. A-MNIST 문제에 대해서 기존 ANN 모델이 98.3%, SNN 모델이 98.66%의 정확도를 보임."]},{"cell_type":"markdown","metadata":{"id":"5ptkXBAHslmV","colab_type":"text"},"source":["### - 개념"]},{"cell_type":"markdown","metadata":{"id":"Ju9uCgmtsn72","colab_type":"text"},"source":["1. 인공신경망과의 다른점: 시간 축이 존재 <br>\n","2. 시간에 따라 뉴런의 값(내부 상태)가 지속적으로 변한다. <br>\n","3. 실제 두뇌를 보다 유사하게 모사하기 위해 '역치'와 '스파이크' 개념을 사용"]},{"cell_type":"markdown","metadata":{"id":"c5hN6TEwwH70","colab_type":"text"},"source":["![대체 텍스트](https://drive.google.com/uc?id=13edZlR4r9Er9CN5UJlF5bb6ptJfGsJtz)"]},{"cell_type":"markdown","metadata":{"id":"YAJCX1T-tLXQ","colab_type":"text"},"source":["### - 작동 방법"]},{"cell_type":"markdown","metadata":{"id":"Ott3ngpTtNuP","colab_type":"text"},"source":["1. 기본적인 구조: ANN과 동일(입력 층, 히든 층, 출력 층) <br>\n","2. 각 뉴런은 \"내부 상태(V)\" 변수를 하나씩 가지고 있다. <br>\n","V: 기본적으로 0, threshold 이하의 실수 값. <br>\n","3. 입력 뉴런은 threshold이상의 신호 시 spike 발생."]},{"cell_type":"markdown","metadata":{"id":"xUL1kmV09bKt","colab_type":"text"},"source":["### - 한계점"]},{"cell_type":"markdown","metadata":{"id":"OQX107729dLt","colab_type":"text"},"source":["1. 학습 방법에 대해 자료가 충분하지 않다.\n","2. 미분 방정식에 대한 시뮬레이션을 필요로 하기 때문에 많은 계산을 필요로 합니다.\n","3. gradient descent를 사용해서 학습시킬 수 없으므로, 새로운 감독 학습 방법을 개발하여야 한다."]},{"cell_type":"markdown","metadata":{"id":"hHvRFzB_--7N","colab_type":"text"},"source":["### - 잘 설명해주는 site"]},{"cell_type":"markdown","metadata":{"id":"IxVjRRGt_BaR","colab_type":"text"},"source":["https://github.com/guillaume-chevalier/Spiking-Neural-Network-SNN-with-PyTorch-where-Backpropagation-engenders-STDP/blob/master/Spiking%20Neural%20Networks%20with%20PyTorch.ipynb"]},{"cell_type":"markdown","metadata":{"id":"WGUE9Jf0rgNv","colab_type":"text"},"source":["* Reference\n","1. https://tykimos.github.io/2019/01/30/Keras_to_SNN/\n","2. https://junis3.tistory.com/34\n"]},{"cell_type":"markdown","metadata":{"id":"qZ_XSUi1azfF","colab_type":"text"},"source":["## 4. Manifold learning"]}]}