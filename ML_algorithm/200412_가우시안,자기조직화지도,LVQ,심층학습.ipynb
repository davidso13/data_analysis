{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"200412_가우시안,자기조직화지도,LVQ,심층학습.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOIn7AzKQUGSHlwiWVROsRn"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Xj1HodWjcH5R","colab_type":"text"},"source":["##200412_가우시안,자기조직화지도,LVQ,심층학습.ipynb"]},{"cell_type":"markdown","metadata":{"id":"faQ2qoHEcH74","colab_type":"text"},"source":["### 1. 가우시안 혼합 모델(Gaussian Mixture Model)"]},{"cell_type":"markdown","metadata":{"id":"0bGzmi31cpto","colab_type":"text"},"source":["(1) Gaussian 분포가 K개 혼합된 cluster 알고리즘<br>\n","(2) Mixture Model: 전체 분포에서 하위 분포가 존재한다는 모델(데이터가 모수를 갖는 여러 개의 분포로부터 생성되었다고 가정)<br>\n","(3) 가우시안 확률 분포의 한계: 데이터들의 평균을 중심으로 하나의 그룹으로 뭉쳐있는 unimodel한 형태만 표현 가능 -> 한계점 완화하기 위해 GMM 모델 사용.<br>\n","(4) 사용 분야: 데이터마이닝, 패턴 인식, 머신 러닝, 통계분석 등에 광범위하게 사용 <br><br>\n","\n","(5) EM 알고리즘을 통해 모델의 파라미터 구함.\n","- 주어진 데이터가 어디 가우시안에서 생선된 데이터인지?<br>\n","(KNOW: K개의 가우시안에서 생성된 데이터 + 각 가우시안이 처음 선택될 확률과 가우시안의 parameter) : M 단계\n","- 가우시안이 선택될 확률과 가우시안들의 parameter 추정<br>\n","(KNOW: K개의 가우시안에서 생성된 데이터 + 각 데이터가 어떤 가우시안에서 생성되었는지) : E 단계\n","- 위의 2과정을 계속해서 반복하고, parameter가 더 이상 변하지 않을 때 반복을 stop하면, 우리가 원하는 가우시안 parameter, 선택될 확률, 그리고 데이터가 어디 가우시안에 속했는지를 알 수 있다."]},{"cell_type":"code","metadata":{"id":"MHPndTB7cp2I","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","from sklearn import cluster, datasets, mixture\n","import numpy as np\n","from scipy.stats import multivariate_normal\n","from sklearn.datasets import make_spd_matrix\n","plt.rcParams[\"axes.grid\"] = False\n","\n","# define the number of samples to be drawn\n","n_samples = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"25xYNv16NwgG","colab_type":"code","outputId":"0dcc0ac4-faec-4fed-b116-f3dfab473064","executionInfo":{"status":"ok","timestamp":1586658514727,"user_tz":-540,"elapsed":1112,"user":{"displayName":"david song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil3Fx21Bq7GBVRGaEz4SijRJXcHDbCf-XMuKGf6Q=s64","userId":"03748752257933305700"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# define the mean points for each of the systhetic cluster centers\n","t_means = [[8.4, 8.2], [1.4, 1.6], [2.4, 5.4], [6.4, 2.4]]\n","\n","# for each cluster center, create a Positive semidefinite convariance matrix\n","t_covs = []\n","for s in range(len(t_means)):\n","  t_covs.append(make_spd_matrix(2))\n","\n","X = []\n","for mean, cov in zip(t_means,t_covs):\n","  x = np.random.multivariate_normal(mean, cov, n_samples)\n","  X += list(x)\n","  \n","X = np.array(X)\n","np.random.shuffle(X)\n","print(\"Dataset shape:\", X.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dataset shape: (400, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ETtrobK3N2z4","colab_type":"code","outputId":"f24ec64e-faf8-45b1-f438-2adde6e73301","executionInfo":{"status":"ok","timestamp":1586658517999,"user_tz":-540,"elapsed":1173,"user":{"displayName":"david song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil3Fx21Bq7GBVRGaEz4SijRJXcHDbCf-XMuKGf6Q=s64","userId":"03748752257933305700"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Create a grid for visualization purposes \n","x = np.linspace(np.min(X[...,0])-1,np.max(X[...,0])+1,100)\n","y = np.linspace(np.min(X[...,1])-1,np.max(X[...,1])+1,80)\n","X_,Y_ = np.meshgrid(x,y)\n","pos = np.array([X_.flatten(),Y_.flatten()]).T\n","print(pos.shape)\n","print(np.max(pos[...,1]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(8000, 2)\n","11.334920050884445\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M0lrkTctN8SX","colab_type":"code","outputId":"78a4b1f9-808d-4060-c825-9e5edd33663d","executionInfo":{"status":"ok","timestamp":1586658535336,"user_tz":-540,"elapsed":1007,"user":{"displayName":"david song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil3Fx21Bq7GBVRGaEz4SijRJXcHDbCf-XMuKGf6Q=s64","userId":"03748752257933305700"}},"colab":{"base_uri":"https://localhost:8080/","height":106}},"source":["# define the number of clusters to be learned\n","k = 4\n","\n","# create and initialize the cluster centers and the weight paramters\n","weights = np.ones((k)) / k\n","means = np.random.choice(X.flatten(), (k,X.shape[1]))\n","print(means)\n","print(weights)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[8.34239963 5.62003589]\n"," [0.53090515 2.1129898 ]\n"," [9.42645314 7.34745668]\n"," [0.77751233 7.80568938]]\n","[0.25 0.25 0.25 0.25]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0mTyU2GuOAr9","colab_type":"code","outputId":"e0cf2b28-0420-45c8-bf4c-925bf2637130","executionInfo":{"status":"ok","timestamp":1586658546584,"user_tz":-540,"elapsed":956,"user":{"displayName":"david song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil3Fx21Bq7GBVRGaEz4SijRJXcHDbCf-XMuKGf6Q=s64","userId":"03748752257933305700"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# create and initialize a Positive semidefinite convariance matrix \n","cov = []\n","for i in range(k):\n","  cov.append(make_spd_matrix(X.shape[1]))\n","cov = np.array(cov)\n","print(cov.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(4, 2, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H6D7lHFsODD9","colab_type":"code","outputId":"a231ab2a-7459-4fc3-dd11-aaf40b16ec41","executionInfo":{"status":"ok","timestamp":1586658577246,"user_tz":-540,"elapsed":18082,"user":{"displayName":"david song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil3Fx21Bq7GBVRGaEz4SijRJXcHDbCf-XMuKGf6Q=s64","userId":"03748752257933305700"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1N5T5yL15NRA7Xp2WwJrnYVRloNK6RxB9"}},"source":["colors = ['tab:blue', 'tab:orange', 'tab:green', 'magenta', 'yellow', 'red', 'brown', 'grey']\n","eps=1e-8\n","\n","# run GMM for 40 steps\n","for step in range(40):\n","\n","  # visualize the learned clusters\n","  if step % 1 == 0:\n","    plt.figure(figsize=(12,int(8)))\n","    plt.title(\"Iteration {}\".format(step))\n","    axes = plt.gca()\n","    \n","    likelihood = []\n","    for j in range(k):\n","      likelihood.append(multivariate_normal.pdf(x=pos, mean=means[j], cov=cov[j]))\n","    likelihood = np.array(likelihood)\n","    predictions = np.argmax(likelihood, axis=0)\n","    \n","    for c in range(k):\n","      pred_ids = np.where(predictions == c)\n","      plt.scatter(pos[pred_ids[0],0], pos[pred_ids[0],1], color=colors[c], alpha=0.2, edgecolors='none', marker='s')\n","    \n","    plt.scatter(X[...,0], X[...,1], facecolors='none', edgecolors='grey')\n","    \n","    for j in range(k):\n","      plt.scatter(means[j][0], means[j][1], color=colors[j])\n","\n","    #plt.savefig(\"img_{0:02d}\".format(step), bbox_inches='tight')\n","    plt.show()\n","\n","  likelihood = []\n","  # Expectation step\n","  for j in range(k):\n","    likelihood.append(multivariate_normal.pdf(x=X, mean=means[j], cov=cov[j]))\n","  likelihood = np.array(likelihood)\n","  assert likelihood.shape == (k, len(X))\n","    \n","  b = []\n","  # Maximization step \n","  for j in range(k):\n","    # use the current values for the parameters to evaluate the posterior\n","    # probabilities of the data to have been generanted by each gaussian\n","    b.append((likelihood[j] * weights[j]) / (np.sum([likelihood[i] * weights[i] for i in range(k)], axis=0)+eps))\n","\n","    # updage mean and variance\n","    means[j] = np.sum(b[j].reshape(len(X),1) * X, axis=0) / (np.sum(b[j]+eps))\n","    cov[j] = np.dot((b[j].reshape(len(X),1) * (X - means[j])).T, (X - means[j])) / (np.sum(b[j])+eps)\n","\n","    # update the weights\n","    weights[j] = np.mean(b[j])\n","    \n","    assert cov.shape == (k, X.shape[1], X.shape[1])\n","    assert means.shape == (k, X.shape[1])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"e_MqV8dhI5XU","colab_type":"text"},"source":["* Reference <br>\n","1. http://blog.naver.com/PostView.nhn?blogId=kmkim1222&logNo=10187825620 <br>\n","2. https://3months.tistory.com/154 <br>\n","3. https://colab.research.google.com/drive/1Eb-G95_dd3XJ-0hm2qDqdtqMugLkSYE8 <br>"]},{"cell_type":"markdown","metadata":{"id":"3F_4NS5ZcH-Z","colab_type":"text"},"source":["###2. 자기조직화지도(SOM, Self-Organizing Map)"]},{"cell_type":"markdown","metadata":{"id":"-PTSBqSDcqYB","colab_type":"text"},"source":["(1) 대뇌피질의 시각피질을 모델화한 인공신경망 <br>\n","(2) 고차원 데이터를 사람이 볼 수 있는 2차원, 3차원 격자에 대응하도록 인경신공망과 유사한 방식의 학습을 통해 군지을 도출해내는 기법 -> 고차원의 데이터 원공간에서 유사한 개체들은 저차원에 인접한 격자들과 연결. <br>\n","(3) Winning node: 임의의 n차원 입력벡터가 들어왔을 대 가장 가까운 격자벡터 -> 군집화 <br>\n","(4) 같은 격자에 할당된 입력벡터라 하더라도 winning node와 거리가 다름. -> 멀고 가까움 표시하면, 고차원 공간의 데이터를 차원 축소 가능. <br>\n","(5) 사용 분야: 차원축소(dimensionality reduction), 군집화(clustering)\n","<br><Br>\n","\n","![대체 텍스트](https://i.imgur.com/ZsAdHxT.png)\n","![대체 텍스트](https://i.imgur.com/EE8NF6J.png)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zF13T0eyU5Wp","colab_type":"text"},"source":["### - SOM 학습방법"]},{"cell_type":"markdown","metadata":{"id":"0_s349RSU-c6","colab_type":"text"},"source":["(1) 검정색: 원데이터(고차원), 연두색: 격자벡터(저차원) <br>\n","(2) 격자벡터의 위치를 랜덤으로 초기화 <br>\n","(3) 입력층과 가장 가까운 노드를 찾는다(Winning node). <br>\n","(4) Winning node 결정 이후, 반복된 학습을 통해 연결강도(W)가 결정.\n","- Wnew = Wold + a(X-Wold) <br>\n","\n","(5) 구성\n","- vector of nodes for input\n","- array of nodes as output map\n","- a matrix of connections <br>\n","\n","![대체 텍스트](https://i.imgur.com//eHUVAtr.png)\n"]},{"cell_type":"code","metadata":{"id":"hEk0sSto1Xdr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EzFKguDTPq1b","colab_type":"text"},"source":["* Reference <br>\n","1. https://ratsgo.github.io/machine%20learning/2017/05/01/SOM/ <br>\n","2. http://www.incodom.kr/Self-organizing_map%28SOR%29 <br>\n"]},{"cell_type":"markdown","metadata":{"id":"tn42O9L7cIBI","colab_type":"text"},"source":["###3. LVQ(Learning Vector Quantization)"]},{"cell_type":"markdown","metadata":{"id":"yXoTpRDqcrK6","colab_type":"text"},"source":["(1) 입력 벡터를 가장 유사한 참조 벡터로 군집화하는 인공신경망 <br>\n","(2) 참조 벡터(reference vector): 경쟁학습을 통해 형성되는 입력 벡터의 그룹 <br>\n","(3) 유클리드 거리 기반의 경쟁학습을 통해 클러스터의 중심을 수정하며 학습 진행"]},{"cell_type":"markdown","metadata":{"id":"akAVvvRKuIJH","colab_type":"text"},"source":["### - LVQ 동작 방법"]},{"cell_type":"markdown","metadata":{"id":"mpZMbgHDuK8S","colab_type":"text"},"source":["(1) 클러스터(참조 벡터)의 수, 학습률 설정 <br>\n","(2) 클러스터의 중심을 0 ~ 1의 임의의 값으로 초기화. <br>\n","(3) k를 0으로 초기화 <br>\n","(4) x_k를 인공신경망의 입력으로 설정. <br>\n","(5) 경쟁학습을 통해 클러스터를 선택 <br>\n","(6) 선택된 클러스터의 중심을 수정 <br>\n","(7-1) 학습한 결과가 종료 조건을 만족 -> 알고리즘 종료 <br>\n","(7-2) 종료 조건 만족X -> k=k+1, (4)부터 다시 수행"]},{"cell_type":"markdown","metadata":{"id":"8uf6OFk64gw0","colab_type":"text"},"source":["### - SOM과 LVQ의 차이점"]},{"cell_type":"markdown","metadata":{"id":"WdE_fVvm4khQ","colab_type":"text"},"source":["- SOM: preserve the data topology by mapping similar data items to same cell on the grid. <br> 유사 데이터를 같은 셀에 맵핑하여 유사성 유지. <br><br>\n","- LVQ: don't take into account data topology. -> use pre-assigned cluster labels to data items. <br> 데이터 유사성을 고려하지 않으므로 미리 데이터 아이템에 클러스터 할당"]},{"cell_type":"code","metadata":{"id":"D1qBXN_LcrSf","colab_type":"code","colab":{}},"source":["\n","import numpy as np\n","\n","# train_lvq: trains an lvq system using the given training data and\n","# corresponding labels. Run the desired number of epochs using the\n","# given learning rate. Optional validation set to monitor performance.\n","def train_lvq(data, labels, num_epochs, learning_rate, validation_data=None, validation_labels=None):\n","    # Get unique class labels.\n","    num_dims = data.shape[1]\n","    labels = labels.astype(int)\n","    unique_labels = list(set(labels))\n","\n","    num_protos = len(unique_labels)\n","    prototypes = np.empty((num_protos, num_dims))\n","    proto_labels = []\n","\n","    # Initialize prototypes using class means.\n","    for i in unique_labels:\n","        class_data = data[labels == i, :]\n","\n","        # Compute class mean.\n","        mean = np.mean(class_data, axis=0)\n","\n","        prototypes[i] = mean\n","        proto_labels.append(i)\n","\n","    # Loop through data set.\n","    for epoch in range(0, num_epochs):\n","        for fvec, lbl in zip(data, labels):\n","            # Compute distance from each prototype to this point\n","            distances = list(np.sum(np.subtract(fvec, p)**2) for p in prototypes)\n","            min_dist_index = distances.index(min(distances))\n","\n","            # Determine winner prototype.\n","            winner = prototypes[min_dist_index]\n","            winner_label = proto_labels[min_dist_index]\n","\n","            # Push or repel the prototype based on the label.\n","            if winner_label == lbl:\n","                sign = 1\n","            else:\n","                sign = -1\n","\n","            # Update winner prototype\n","            prototypes[min_dist_index] = np.add(prototypes[min_dist_index], np.subtract(fvec, winner) * learning_rate * sign)\n","\n","        # Use validation set to test performance.\n","        val_err = 0\n","        if validation_labels is not None:\n","            for fvec, lbl in zip(validation_data, validation_labels):\n","                distances = list(np.sum(np.subtract(fvec, p) ** 2) for p in prototypes)\n","                min_dist_index = distances.index(min(distances))\n","\n","                # Determine winner prototype label\n","                winner_label = proto_labels[min_dist_index]\n","\n","                # Check if labels match\n","                if not winner_label == lbl:\n","                    val_err = val_err + 1\n","\n","            val_err = val_err / len(validation_labels)\n","            print(\"Epoch \" + str(epoch) + \". Validation error: \" + str(val_err))\n","        else:\n","            print(\"Epoch \" + str(epoch))\n","\n","\n","    return (prototypes, proto_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t1LZSJ-T1yNi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":216},"outputId":"019c43dd-ecd6-4b1c-dbeb-a0799a508fd5","executionInfo":{"status":"ok","timestamp":1586668992820,"user_tz":-540,"elapsed":8009,"user":{"displayName":"david song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil3Fx21Bq7GBVRGaEz4SijRJXcHDbCf-XMuKGf6Q=s64","userId":"03748752257933305700"}}},"source":["!pip install neurolab"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Collecting neurolab\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/fd/47a9a39158b461b6b862d64c0ad7f679b08ed6d316744299f0db89066342/neurolab-0.3.5.tar.gz (645kB)\n","\r\u001b[K     |▌                               | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 836kB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 1.2MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 1.0MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 1.2MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71kB 1.4MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 1.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 1.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 143kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 194kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 215kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 266kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 286kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 317kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 337kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 358kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 378kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 389kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 399kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 409kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 430kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 440kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 450kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 460kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 471kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 481kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 501kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 512kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 532kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 552kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 573kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 1.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 593kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 1.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 614kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 1.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 634kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 1.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 655kB 1.4MB/s \n","\u001b[?25hBuilding wheels for collected packages: neurolab\n","  Building wheel for neurolab (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for neurolab: filename=neurolab-0.3.5-cp36-none-any.whl size=22180 sha256=60a6cc382265f5124c41eacd81811b1e1795a7771556ed39d8da20ca147df0b2\n","  Stored in directory: /root/.cache/pip/wheels/c6/8f/37/32ab1cf4d601dc0bc49d7241012a4292db4b343bebff5b68e6\n","Successfully built neurolab\n","Installing collected packages: neurolab\n","Successfully installed neurolab-0.3.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h-q-bika1l97","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":370},"outputId":"79675362-62d7-413d-b542-9843ea4aef6b","executionInfo":{"status":"error","timestamp":1586669127349,"user_tz":-540,"elapsed":1822,"user":{"displayName":"david song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil3Fx21Bq7GBVRGaEz4SijRJXcHDbCf-XMuKGf6Q=s64","userId":"03748752257933305700"}}},"source":["\"\"\"\n","Example of use LVQ network\n","==========================\n","\n","\"\"\"\n","import numpy as np\n","import neurolab as nl\n","\n","# Create train samples\n","input = np.array([[-3, 0], [-2, 1], [-2, -1], [0, 2], [0, 1], [0, -1], [0, -2], \n","                                                        [2, 1], [2, -1], [3, 0]])\n","target = np.array([[1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], \n","                                                        [1, 0], [1, 0], [1, 0]])\n","\n","# Create network with 2 layers: 4 neurons in input layer(Competitive)\n","# and 2 neurons in output layer(liner)\n","net = nl.net.newlvq(nl.tool.minmax(input), 4, [0.6, 0.4])\n","# Train network\n","error = net.train(input, target, epochs=1000, goal=-1)\n","\n","# Plot result\n","import pylab as pl\n","xx, yy = np.meshgrid(np.arange(-3, 3.4, 0.2), np.arange(-3, 3.4, 0.2))\n","xx.shape = xx.size, 1\n","yy.shape = yy.size, 1\n","i = np.concatenate((xx, yy), axis=1)\n","o = net.sim(i)\n","grid1 = i[o[:, 0]>0]\n","grid2 = i[o[:, 1]>0]\n","\n","class1 = input[target[:, 0]>0]\n","class2 = input[target[:, 1]>0]\n","\n","pl.plot(class1[:,0], class1[:,1], 'bo', class2[:,0], class2[:,1], 'go')\n","pl.plot(grid1[:,0], grid1[:,1], 'b.', grid2[:,0], grid2[:,1], 'gx')\n","pl.axis([-3.2, 3.2, -3, 3])\n","pl.xlabel('Input[:, 0]')\n","pl.ylabel('Input[:, 1]')\n","pl.legend(['class 1', 'class 2', 'detected class 1', 'detected class 2'])\n","pl.show()"],"execution_count":26,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-fcce8369eabb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Create network with 2 layers: 4 neurons in input layer(Competitive)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# and 2 neurons in output layer(liner)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewlvq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# Train network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/neurolab/net.py\u001b[0m in \u001b[0;36mnewlvq\u001b[0;34m(minmax, cn0, pc)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mlayer_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     net = Net(minmax, cn1, [layer_inp, layer_out],\n\u001b[1;32m    181\u001b[0m                             [[-1], [0], [1]], train.train_lvq, error.MSE())\n","\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"]}]},{"cell_type":"code","metadata":{"id":"atRJV_d53Bnw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6ce4bcff-6210-4ff7-b701-f082db197387","executionInfo":{"status":"ok","timestamp":1586669357797,"user_tz":-540,"elapsed":52049,"user":{"displayName":"david song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil3Fx21Bq7GBVRGaEz4SijRJXcHDbCf-XMuKGf6Q=s64","userId":"03748752257933305700"}}},"source":["!pip install neupy"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Collecting neupy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/be/19082cbe9a6c76dd909255341587f0b487cd3e9d32d44debda013d2accd1/neupy-0.8.2-py2.py3-none-any.whl (226kB)\n","\u001b[K     |████████████████████████████████| 235kB 1.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from neupy) (1.18.2)\n","Collecting tensorflow<1.14.0,>=1.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/d3/651f95288a6cd9094f7411cdd90ef12a3d01a268009e0e3cd66b5c8d65bd/tensorflow-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (92.6MB)\n","\u001b[K     |████████████████████████████████| 92.6MB 60kB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from neupy) (2.10.0)\n","Collecting graphviz==0.5.1\n","  Downloading https://files.pythonhosted.org/packages/55/8d/18e45d3f57adfde20ac831a5ba7144b9e643185e05eb0a02b6f22d076752/graphviz-0.5.1-py2.py3-none-any.whl\n","Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from neupy) (3.2.1)\n","Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from neupy) (1.4.1)\n","Collecting progressbar2==3.34.3\n","  Downloading https://files.pythonhosted.org/packages/87/31/b984e17bcc7491c1baeda3906fe3abc14cb5cd5dbd046ab46d9fc7a2edfd/progressbar2-3.34.3-py2.py3-none-any.whl\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (0.3.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (1.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (1.0.8)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (0.9.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (1.12.0)\n","Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n","\u001b[K     |████████████████████████████████| 368kB 39.9MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (0.34.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (3.10.0)\n","Collecting tensorboard<1.14.0,>=1.13.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 34.8MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (1.28.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<1.14.0,>=1.10.1->neupy) (0.8.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->neupy) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->neupy) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->neupy) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->neupy) (1.2.0)\n","Requirement already satisfied: python-utils>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2==3.34.3->neupy) (2.4.0)\n","Collecting mock>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<1.14.0,>=1.10.1->neupy) (46.1.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow<1.14.0,>=1.10.1->neupy) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow<1.14.0,>=1.10.1->neupy) (3.2.1)\n","Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow, graphviz, progressbar2, neupy\n","  Found existing installation: tensorflow-estimator 2.2.0rc0\n","    Uninstalling tensorflow-estimator-2.2.0rc0:\n","      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n","  Found existing installation: tensorboard 2.2.0\n","    Uninstalling tensorboard-2.2.0:\n","      Successfully uninstalled tensorboard-2.2.0\n","  Found existing installation: tensorflow 2.2.0rc2\n","    Uninstalling tensorflow-2.2.0rc2:\n","      Successfully uninstalled tensorflow-2.2.0rc2\n","  Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","  Found existing installation: progressbar2 3.38.0\n","    Uninstalling progressbar2-3.38.0:\n","      Successfully uninstalled progressbar2-3.38.0\n","Successfully installed graphviz-0.5.1 mock-4.0.2 neupy-0.8.2 progressbar2-3.34.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OsTWLYf423bj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"da252e69-e637-4a35-b3ba-c2e818dd08f3","executionInfo":{"status":"ok","timestamp":1586669388202,"user_tz":-540,"elapsed":1137,"user":{"displayName":"david song","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gil3Fx21Bq7GBVRGaEz4SijRJXcHDbCf-XMuKGf6Q=s64","userId":"03748752257933305700"}}},"source":["import numpy as np\n","from neupy import algorithms\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [2, 2], [1, 2]])\n","y = np.array([0, 0, 0, 1, 1, 1])\n","lvqnet = algorithms.LVQ(n_inputs=2, n_classes=2)\n","lvqnet.train(X, y, epochs=100)\n","lvqnet.predict([[2, 1], [-1, -1]])"],"execution_count":30,"outputs":[{"output_type":"stream","text":["\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r\r                                                                               \r"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array([1, 0])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"uu7fXyCStsmU","colab_type":"text"},"source":["* Reference <br>\n","1. https://untitledtblog.tistory.com/50?category=667127 <br> \n","2. http://facweb.cs.depaul.edu/mobasher/classes/csc426/review/categorization.pdf <br>\n"]},{"cell_type":"markdown","metadata":{"id":"eEKpUqPwcgy4","colab_type":"text"},"source":["###4. 심층학습(DNN, Deep Neural Network)"]},{"cell_type":"markdown","metadata":{"id":"ytG0y6jRcrvY","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"6gDFQ0rocr3o","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}